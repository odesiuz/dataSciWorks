{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n10813440/11490434 [===========================>..] - ETA: 0sx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\n"
                }
            ], 
            "source": "'''Trains a simple convnet on the MNIST dataset.\n\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist # mnist dataset is a test dataset already in Keras lib.\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten # \"Dropout\" is functionality to prevent overfitting. It is modelled as a seperate layer in Keras. \"Flatten\" is a function which reduces the dimensionality of a tensor.\nfrom keras.layers import Conv2D, MaxPooling2D # \"Conv2D\" is a 2 dimensional convolutional layer which perfectly fits image data. \"MaxPooling2D\" is a pooling layer using the max pooling funtion in 2D\nfrom keras import backend as K # K is a function in keras backend.\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data() # We load the data. load_data is good because it returns a tupple of 4 numpy arrays. y_train = target vector, x_train input vector.\n\nif K.image_data_format() == 'channels_first': # This defines the way the colour information is encoded in the image.\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) # the 1 here signifies its a monochrome image and not a color image.\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255 # we normalize the data by dividing by 255 to obtain a range between 0 and 1\nx_test /= 255\nprint('x_train shape:', x_train.shape) # We print to verify if the shape is correct.\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes) # The label is recorded as a singular mention with variables between zero and 9, we transform this to \"one hot\" encoded vector using the Keras to_categorical function\ny_test = keras.utils.to_categorical(y_test, num_classes)"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n\n#Please construct the following neural network and report accuracy after training\n#Layer 1: 2D Convolution with 32 hidden neurons, kernel 3 by 3, relu activation (Note: input_shape is given)\n#Layer 2: 2D Convolution with 64 hidden neurons, kernel 3 by 3, relu activation \n#Layer 3: 2D MaxPooling, pool_size 2 by 2\n#Layer 4: 25% Dropout\n#Layer 5: Flatten (Hint: model.add(Flatten()))\n#Layer 6: Fully connected layer with 128 neurons, relu activation\n#Layer 7: 50% Dropout\n#Layer 8 Softmax Output Layer according to the problem (output vector)\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', input_shape = input_shape)) # Layer 1\nmodel.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu')) # Layer 2\nmodel.add(MaxPooling2D(pool_size = (2,2))) # Layer 3\nmodel.add(Dropout(0.25)) # Layer 4\nmodel.add(Flatten()) # Layer 5 Flatten does 28 * 28 to get 784 neurons\nmodel.add(Dense(128, activation = 'relu')) # Layer 6\nmodel.add(Dropout(0.5)) # Layer 7\nmodel.add(Dense(num_classes, activation = 'softmax')) # Layer 8\n\n\n"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 386s - loss: 1.8714 - acc: 0.3660 - val_loss: 0.7832 - val_acc: 0.7608\nEpoch 2/12\n60000/60000 [==============================] - 383s - loss: 1.4104 - acc: 0.5217 - val_loss: 0.7720 - val_acc: 0.8141\nEpoch 3/12\n60000/60000 [==============================] - 381s - loss: 1.3896 - acc: 0.5405 - val_loss: 0.8451 - val_acc: 0.8013\nEpoch 4/12\n60000/60000 [==============================] - 380s - loss: 1.3910 - acc: 0.5485 - val_loss: 0.7281 - val_acc: 0.8216\nEpoch 5/12\n60000/60000 [==============================] - 380s - loss: 1.3565 - acc: 0.5741 - val_loss: 0.8053 - val_acc: 0.8116\nEpoch 6/12\n60000/60000 [==============================] - 379s - loss: 1.3788 - acc: 0.5787 - val_loss: 1.0700 - val_acc: 0.7941\nEpoch 7/12\n60000/60000 [==============================] - 380s - loss: 1.3685 - acc: 0.5812 - val_loss: 0.7799 - val_acc: 0.8283\nEpoch 8/12\n60000/60000 [==============================] - 379s - loss: 1.3586 - acc: 0.5883 - val_loss: 0.8695 - val_acc: 0.8491\nEpoch 9/12\n60000/60000 [==============================] - 381s - loss: 1.3701 - acc: 0.5971 - val_loss: 0.8331 - val_acc: 0.8492\nEpoch 10/12\n60000/60000 [==============================] - 380s - loss: 1.3771 - acc: 0.6031 - val_loss: 0.9408 - val_acc: 0.8335\nEpoch 11/12\n60000/60000 [==============================] - 379s - loss: 1.4172 - acc: 0.6117 - val_loss: 1.0412 - val_acc: 0.7995\nEpoch 12/12\n60000/60000 [==============================] - 380s - loss: 1.4101 - acc: 0.6234 - val_loss: 0.9030 - val_acc: 0.8502\nTest loss: 0.9029567852735519\nTest accuracy: 0.8502\n"
                }
            ], 
            "source": "# Our loss fn is categorical_crossentropy which is a very good fit for multiclass model task in Keras\n# We use an optimizer as Adadelta which turns out to converge best given the dataset that we know the topology \nmodel.compile(loss=keras.losses.categorical_crossentropy, \n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\n#some learners constantly reported 502 errors in Watson Studio. \n#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n#This is a workaround to limit resource consumption\n\nfrom keras import backend as K\n\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.14", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}