{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'''Trains a simple convnet on the MNIST dataset.\n\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist # mnist dataset is a test dataset already in Keras lib.\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten # \"Dropout\" is functionality to prevent overfitting. It is modelled as a seperate layer in Keras. \"Flatten\" is a function which reduces the dimensionality of a tensor.\nfrom keras.layers import Conv2D, MaxPooling2D # \"Conv2D\" is a 2 dimensional convolutional layer which perfectly fits image data. \"MaxPooling2D\" is a pooling layer using the max pooling funtion in 2D\nfrom keras import backend as K # K is a function in keras backend.\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data() # We load the data. load_data is good because it returns a tupple of 4 numpy arrays. y_train = target vector, x_train input vector.\n\nif K.image_data_format() == 'channels_first': # This defines the way the colour information is encoded in the image.\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) # the 1 here signifies its a monochrome image and not a color image.\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255 # we normalize the data by dividing by 255 to obtain a range between 0 and 1\nx_test /= 255\nprint('x_train shape:', x_train.shape) # We print to verify if the shape is correct.\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes) # The label is recorded as a singular mention with variables between zero and 9, we transform this to \"one hot\" encoded vector using the Keras to_categorical function\ny_test = keras.utils.to_categorical(y_test, num_classes)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n\n#Please construct the following neural network and report accuracy after training\n#Layer 1: 2D Convolution with 32 hidden neurons, kernel 3 by 3, relu activation (Note: input_shape is given)\n#Layer 2: 2D Convolution with 64 hidden neurons, kernel 3 by 3, relu activation \n#Layer 3: 2D MaxPooling, pool_size 2 by 2\n#Layer 4: 25% Dropout\n#Layer 5: Flatten (Hint: model.add(Flatten()))\n#Layer 6: Fully connected layer with 128 neurons, relu activation\n#Layer 7: 50% Dropout\n#Layer 8 Softmax Output Layer according to the problem (output vector)\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', input_shape = input_shape)) # Layer 1\nmodel.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu')) # Layer 2\nmodel.add(MaxPooling2D(pool_size = (2,2))) # Layer 3\nmodel.add(Dropout(0.25)) # Layer 4\nmodel.add(Flatten()) # Layer 5\nmodel.add(Dense(128, activation = 'relu')) # Layer 6\nmodel.add(Dropout(0.5)) # Layer 7\nmodel.add(Dense(num_classes, activation = 'softmax')) # Layer 8\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\n#some learners constantly reported 502 errors in Watson Studio. \n#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n#This is a workaround to limit resource consumption\n\nfrom keras import backend as K\n\nK.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.14", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}